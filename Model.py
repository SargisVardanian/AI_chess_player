# chess_model_reworked.py
# –ü–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ —Å –∫–æ–Ω–≤–æ–ª—é—Ü–∏—è–º–∏ ‚Üí dense ‚Üí self‚Äëattention,
# –≥–¥–µ token_in –≤—Ö–æ–¥–∏—Ç –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∫–∞–∫ –≤—Ç–æ—Ä–æ–π —Ç–æ–∫–µ–Ω, –∞ token_out –±–µ—Ä—ë—Ç—Å—è –∏–∑ seq[:,1,:]

import torch
import torch.nn as nn
import torch.nn.functional as F
import chess

device = torch.device(
    'mps' if torch.backends.mps.is_available()
    else 'cuda' if torch.cuda.is_available()
    else 'cpu'
)

# ------------------------------------------------------------------
# üî∏ Inception‚Äë–ø–æ–¥–æ–±–Ω—ã–π –º–Ω–æ–≥–æ‚Äë–º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–ª–æ–∫ (3√ó3,‚ÄØ5√ó5,‚ÄØ7√ó7) –±–µ–∑ BN
# ------------------------------------------------------------------
class MultiScaleConvBlock(nn.Module):
    def __init__(self, in_channels: int,
                 c3: int = 64, c5: int = 64, c7: int = 64):
        super().__init__()
        self.branch3 = nn.Sequential(
            nn.Conv2d(in_channels, c3, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(c3, c3, 3, padding=1),
            nn.ReLU(inplace=True),
        )
        self.branch5 = nn.Sequential(
            nn.Conv2d(in_channels, c5, 5, padding=2),
            nn.ReLU(inplace=True),
            nn.Conv2d(c5, c5, 5, padding=2),
            nn.ReLU(inplace=True),
        )
        self.branch7 = nn.Sequential(
            nn.Conv2d(in_channels, c7, 7, padding=3),
            nn.ReLU(inplace=True),
            nn.Conv2d(c7, c7, 7, padding=3),
            nn.ReLU(inplace=True),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return torch.cat([
            self.branch3(x),
            self.branch5(x),
            self.branch7(x)
        ], dim=1)  # (B, 192, 8, 8)


# ------------------------------------------------------------------
# üî∏ –ü—Ä–æ—Å—Ç–æ–π residual –±–ª–æ–∫ 3√ó3 ‚Üí 3√ó3 –±–µ–∑ BN
# ------------------------------------------------------------------
class ResidualBlock(nn.Module):
    def __init__(self, channels: int, dropout: float = 0.0):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.dropout = nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out = F.relu(x)
        out = self.conv1(out)
        out = F.relu(out)
        out = self.dropout(self.conv2(out))
        return x + out  # skip‚Äëconnection


# ------------------------------------------------------------------
# üî∏ ChessModel
# ------------------------------------------------------------------
class ChessModel(nn.Module):
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç:
        board_x  ‚Äì (B,65,16)     —Ç–µ–Ω–∑–æ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è
        token_in ‚Äì (B,512)       —Å–∫—Ä—ã—Ç—ã–π —Ç–æ–∫–µ–Ω –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ö–æ–¥–∞
        mask     ‚Äì (B,4096) bool –º–∞—Å–∫–∞ –ª–µ–≥–∞–ª—å–Ω—ã—Ö —Ö–æ–¥–æ–≤
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        logits     ‚Äì (B,4096)
        token_out  ‚Äì (B,512)     –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Å–∫—Ä—ã—Ç—ã–π —Ç–æ–∫–µ–Ω
    """
    def __init__(self,
                 in_channels: int = 256,
                 token_dim: int = 512,
                 n_heads: int = 16,
                 n_attn_layers: int = 6,
                 n_res_blocks: int = 4,
                 dropout: float = 0.):
        super().__init__()
        self.token_dim = token_dim
        self.dense_dim = token_dim // 4
        self.conv_dim  = 3 * token_dim // 4

        # 1) conv‚Äë–≤–µ—Ç–∫–∞ + MultiScale + residual tower
        self.stem = nn.Sequential(
            nn.Conv2d(16, in_channels, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        self.ms = MultiScaleConvBlock(
            in_channels, self.dense_dim, self.dense_dim, self.dense_dim
        )
        self.squeeze   = nn.Conv2d(self.conv_dim, self.conv_dim, 1)
        self.res_tower = nn.Sequential(
            *[ResidualBlock(self.conv_dim, dropout) for _ in range(n_res_blocks)]
        )

        # 2) dense‚Äë–≤–µ—Ç–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–ª–µ—Ç–∫–∏
        self.cell_dense = nn.Sequential(
            nn.Linear(16, self.dense_dim), nn.ReLU(inplace=True),
            nn.Linear(self.dense_dim, self.dense_dim), nn.ReLU(inplace=True)
        )

        # 3) —Å–ø–µ—Ü‚Äë—Ç–æ–∫–µ–Ω –∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–ª–µ
        self.special_embed = nn.Linear(16, token_dim)

        # 4) Transformer encoder
        enc_layer = nn.TransformerEncoderLayer(
            d_model=token_dim,
            nhead=n_heads,
            dim_feedforward=4*token_dim,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(enc_layer, n_attn_layers)

        # 5) –ø–æ–ª–∏—Ç–∏–∫–∞ –∏ value‚ÄëMLP
        self.repr_mlp = nn.Sequential(
            nn.Linear(token_dim, 2*token_dim), nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(2*token_dim, token_dim), nn.ReLU(inplace=True)
        )
        self.policy_head = nn.Sequential(
            nn.Linear(token_dim, 1024), nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(1024, 4096)
        )

        self.value_head = nn.Sequential(
            nn.Linear(token_dim, 1024), nn.ReLU(),
            nn.Linear(1024, 1024), nn.ReLU(),
            nn.Linear(1024, 1)
        )


    def representation(self,
                       x: torch.Tensor,
                       token_in: torch.Tensor
                       ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        x        ‚Äì (B,65,16)
        token_in ‚Äì (B,512)
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
          h         ‚Äì (B,token_dim) –¥–ª—è –ø–æ–ª–∏—Ç–∏–∫–∏
          token_out ‚Äì (B,token_dim) –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω –∏–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
        """
        B = x.size(0)
        special, cells = x[:, 0, :], x[:, 1:, :]  # (B,16) –∏ (B,64,16)

        # 1) —Å–≤—ë—Ä—Ç–∫–∏ ‚Üí –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feat = cells.view(B, 8, 8, 16).permute(0, 3, 1, 2)  # (B,16,8,8)
        y = self.squeeze(self.ms(self.stem(feat)))         # (B,conv_dim,8,8)
        y = self.res_tower(y)                              # (B,conv_dim,8,8)
        y = y.view(B, self.conv_dim, 64).transpose(1, 2)   # (B,64,conv_dim)

        # 2) dense‚Äë–ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–ª–µ—Ç–æ–∫
        d = self.cell_dense(cells)                         # (B,64,dense_dim)

        # 3) –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤ —Ç–æ–∫–µ–Ω—ã –∫–ª–µ—Ç–æ–∫
        tokens_cells = torch.cat([y, d], dim=-1)           # (B,64,token_dim)

        # 4) –≥–æ—Ç–æ–≤–∏–º –≤—Ö–æ–¥ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
        cls_tok  = self.special_embed(special).unsqueeze(1)   # (B,1,token_dim)
        in_tok   = token_in.unsqueeze(1)                      # (B,1,token_dim)
        # print("in_tok: ", in_tok.shape)
        seq      = torch.cat([cls_tok, in_tok, tokens_cells], dim=1)
        #          ‚îî‚îÄ‚îÄ  index=0   index=1      index=2..65   ‚îÄ‚îÄ‚îò

        # 5) –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ Transformer
        seq = self.transformer(seq)  # (B,66,token_dim)

        # 6) –≤—ã–Ω–∏–º–∞–µ–º –≤—ã—Ö–æ–¥—ã
        h         = self.repr_mlp(seq[:, 0, :])  # –∏–∑ CLS
        token_out = seq[:, 1, :]                 # –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Å–∫—Ä—ã—Ç—ã–π —Ç–æ–∫–µ–Ω
        # print("token_out: ", token_out.shape)
        return h, token_out

    def forward(self,
                board_x: torch.Tensor,
                token_in: torch.Tensor,
                mask: torch.Tensor | None = None
                ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        board_x  ‚Äì (B,65,16)
        token_in ‚Äì (B,512)
        mask     ‚Äì (B,4096)
        """
        h, token_out = self.representation(board_x, token_in)
        logits = self.policy_head(h)
        value = self.value_head(token_out).squeeze(-1)

        if mask is not None:
            logits = logits.masked_fill(~mask, float('-inf'))
        return logits, token_out, value


# ------------------------------------------------------------------
# üî∏ –£—Ç–∏–ª–∏—Ç—ã
# ------------------------------------------------------------------
def index_to_move(idx: int) -> chess.Move:
    return chess.Move(idx // 64, idx % 64)

def evaluate_move(board: chess.Board, mv: chess.Move) -> bool:
    return mv in board.legal_moves


# ------------------------------------------------------------------
# üî∏ –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
# ------------------------------------------------------------------
if __name__ == "__main__":
    model = ChessModel().to(device)
    dummy_x     = torch.zeros((2, 65, 16), device=device)
    dummy_token = torch.zeros((2, 512), device=device)
    logits, tok = model(dummy_x, dummy_token)
    print("logits shape:", logits.shape)    # (2,4096)
    print("token_out shape:", tok.shape)    # (2,512)
    print("‚âà # params:", sum(p.numel() for p in model.parameters()))
